{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_LJO5F2uqAk3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWR75IP5pzVy",
        "outputId": "208d7e9c-2e89-4350-e347-052c2bd84609"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample text\n",
        "text = \" I live in New York, America\"\n",
        "\n",
        "# Tokenization with NLTK\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhcn9Z8QukqO",
        "outputId": "aaf06246-d8c5-4b23-f8f6-ab2e78578609"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'live', 'in', 'New', 'York', ',', 'America']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with NLTK\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQKBH-lCupwI",
        "outputId": "883827a5-0ca7-441f-cd33-86540933e616"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized: ['I', 'live', 'in', 'New', 'York', ',', 'America']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming with NLTK\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed:\", stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saOkhtMluuTk",
        "outputId": "0ee54f06-d372-4fa9-8015-a81e957c7cf2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed: ['i', 'live', 'in', 'new', 'york', ',', 'america']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "nltk.download('stopwords'),\n",
        "filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "print(\"Without Stopwords:\", filtered_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPV_7n9bqEYo",
        "outputId": "41042017-2893-4d6d-c9ce-9c857f705803"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Stopwords: ['I', 'live', 'New', 'York', ',', 'America']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition with spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "print(\"Named Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVOZPACRqJDM",
        "outputId": "00c16b02-feb0-478a-9305-431c0e56c312"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('New York', 'GPE'), ('America', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorization: Bag of Words and TF-IDF\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Machine learning is a core part of NLP.\",\n",
        "    \"NLP is evolving with deep learning models.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# [Natural, Language, core ,Machine ....., models]\n",
        "\n",
        "# sentence 1 : [1,2,0 ...... ]\n",
        "# sentence 2 : [0,, 0, 1, 1 ]\n",
        "# sentence 3 : [0, ]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(texts)\n",
        "print(\"Bag of Words:\", bow_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQQZYMILqOND",
        "outputId": "5318c4ea-033c-40f9-f203-58b76b886e53"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words: [[1 0 0 1 0 1 0 2 0 0 0 1 0 0 0 1 1 1 0]\n",
            " [0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            " [0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "print(\"TF-IDF:\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfAqI-isqWhv",
        "outputId": "79f51454-fbbc-473c-9ef9-2d832ea0574f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF: [[0.30151134 0.         0.         0.30151134 0.         0.30151134\n",
            "  0.         0.60302269 0.         0.         0.         0.30151134\n",
            "  0.         0.         0.         0.30151134 0.30151134 0.30151134\n",
            "  0.        ]\n",
            " [0.         0.41756662 0.         0.         0.         0.\n",
            "  0.31757018 0.         0.31757018 0.41756662 0.         0.\n",
            "  0.31757018 0.41756662 0.41756662 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.41756662 0.         0.41756662 0.\n",
            "  0.31757018 0.         0.31757018 0.         0.41756662 0.\n",
            "  0.31757018 0.         0.         0.         0.         0.\n",
            "  0.41756662]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample text for sentiment analysis\n",
        "texts = [\"I love learning about NLP!\", \"This is a challenging task.\"]\n",
        "\n",
        "# Convert to BERT format\n",
        "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Make predictions\n",
        "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "results = nlp(texts)\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: {text}, Sentiment: {result['label']}, Score: {result['score']:.2f}\")\n",
        "\n",
        "\n",
        "#1 Model Scratch Train (GPU, Data )\n",
        "#2 MOdel FineTune (Specific Data Configure)\n",
        "#3 Model Use\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1qPQXwxq2Hx",
        "outputId": "ce2791a2-7008-4904-f7a4-5d3e87beab6e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love learning about NLP!, Sentiment: LABEL_0, Score: 0.57\n",
            "Text: This is a challenging task., Sentiment: LABEL_0, Score: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06fmNqLbrKOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}